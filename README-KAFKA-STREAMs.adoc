= DEVELOPING with KAFKA STREAMS
:toc:

link:http://kafka.apache.org/10/javadoc[Kafka 10 Javadocs]

== STREAMS DSL

== STREAMS & State

=== Ingredients of state in streams

State is nothing more than the ability to recall information you’ve seen before and connect it to current information. You can utilize state in different ways. 

* *accumulation of values*, provided by the Kafka Streams DSL.
* *Joining of streams* (closely related to the joins performed in database operations eg. such as joining records from the employee and department tables to generate a report on who staffs which departments
in a company.)
* What *state needs to look like* and what the requirements are for using state when we discuss *state stores* in Kafka Streams. 
* Importance of *timestamps* (ensuring you only work with events occurring within a given *time frame* or helping you work with data arriving out of order)


==== Stream Partitioning in Stream

* link:http://kafka.apache.org/10/javadoc/org/apache/kafka/streams/processor/StreamPartitioner.html[StreamPartitioner Javadoc]

* If events for single Customer ID are distributed amongst partitions (ie. no key associated) you will have an issue here (unless you’re using topics with only one partition). Because the key isn’t populated, round-robin assignment means the transactions for a given customer won’t land on the same partition.
* *Placing customer transactions with the same ID on the same partition is important*, because you need to look up records by ID in the state store. Otherwise, you’ll have customers with the same ID spread across different partitions, requiring you to look up the same customer in multiple state stores. _(This statement could be interpreted to mean that each partition has its own state store, but that’s not the case. Partitions are assigned to a *StreamTask* , and *each StreamTask has its own state store*.)_

===== StreamPartitioner

[source java]
----
org.apache.kafka.streams.processor.StreamPartitioner<K, V>


- Determine how records are distributed among the partitions in a Kafka topic. If not specified, the underlying producer's DefaultPartitioner will be used to determine the partition. 
- Kafka topics are divided into one or more partitions. Since each partition must fit on the servers that host it, so using multiple partitions allows the topic to scale beyond a size that will fit on a single machine. 
- Partitions also enable you to use multiple instances of your topology to process in parallel all of the records on the topology's source topics. 
- When a topology is instantiated, each of its sources are assigned a subset of that topic's partitions. That means that only those processors in that topology instance will consume the records from those partitions. 
- In many cases, Kafka Streams will automatically manage these instances, and adjust when new topology instances are added or removed. 
- Some topologies, though, need more control over which records appear in each partition. For example, some topologies that have stateful processors may want all records within a range of keys to always be 
 delivered to and handled by the same topology instance. An upstream topology producing records to that topic can use a custom stream partitioner to precisely and consistently determine to which partition each 
 record should be written. To do this, create a _StreamPartitioner_ implementation, and when you build your topology specify that  custom partitioner when adding a sink for that topic. 
- All StreamPartitioner implementations should be stateless and a pure function so they can be shared across topic and sink nodes.
See Also:
	TopologyBuilder.addSink(String, String, org.apache.kafka.common.serialization.Serializer, org.apache.kafka.common.serialization.Serializer, StreamPartitioner, String)
	TopologyBuilder.addSink(String, String, StreamPartitioner, String)

public class RewardsStreamPartitioner implements StreamPartitioner<String, Purchase> {

    @Override
    public Integer partition(String key, Purchase value, int numPartitions) {
        return value.getCustomerId().hashCode() % numPartitions;
    }
}
----

==== State stores in Kafka Streams

[source java]
----
String rewardsStateStoreName = "rewardsPointsStore";
KeyValueBytesStoreSupplier storeSupplier = Stores.inMemoryKeyValueStore(rewardsStateStoreName);
StoreBuilder<KeyValueStore<String, Integer>> storeBuilder = Stores.keyValueStoreBuilder(storeSupplier, Serdes.String(), Serdes.Integer());
builder.addStateStore(storeBuilder);
----

* KeyValueBytesStoreSupplier
* Stores
** Stores.windowStoreBuilder(WindowBytesStoreSupplier, Serde<K>, Serde<V>)
** Stores.inMemoryKeyValueStore
** Stores.persistentKeyValueStore
** Stores.lruMap
** Stores.persistentSessionStore
* StoreBuilder
** .withCachingEnabled 	(Enable caching on the store.)
** .withLoggingEnabled 	Maintain a changelog for any changes made to the store.
** .withLoggingDisabled Disable the changelog for store built by this {@link StoreBuilder} whici will turn off fault-tolerance for your store. By default the changelog is enabled.
** .logConfig		Returns a Map containing any log configs that will be used when creating the changelog for the {@link StateStore}.
** .loggingEnabled      Return {@code true} if the {@link StateStore} should have logging enabled
** .name                Return the name of this state store builder.
* KeyValueBytesStoreSupplier
** RocksDbKeyValueBytesStoreSupplier
* KeyValueStore
** InMemoryKeyValueStore

[source java]
----
Setting changelog properties
Map<String, String> changeLogConfigs = new HashMap<>();
changeLogConfigs.put("retention.ms","172800000" );
changeLogConfigs.put("retention.bytes", "10000000000");
// to use with a StoreBuilder
storeBuilder.withLoggingEnabled(changeLogConfigs);
// to use with Materialized
Materialized.as(Stores.inMemoryKeyValueStore("foo")
.withLoggingEnabled(changeLogConfigs));

Setting a cleanup policy
Map<String, String> changeLogConfigs = new HashMap<>();
changeLogConfigs.put("retention.ms","172800000" );
changeLogConfigs.put("retention.bytes", "10000000000");
changeLogConfigs.put("cleanup.policy", "compact,delete");
----

==== Joining Kafka streams
Book section 4.4

==== Timestamps in Kafka Streams

Timestamps play a role in key areas of Kafka Streams functionality:
* Joining streams
* Updating a changelog ( KTable API)
* Deciding when the Processor.punctuate method is triggered (Processor API)

In stream processing, you can group timestamps into 3 categories:

1. *Event time:* A timestamp set when the event occurred, usually embedded in the object used to represent the event..
2. *Ingestion time:*  A timestamp set when the data first enters the data processing pipeline. You can consider the timestamp set by the Kafka broker (assuming a configuration setting of LogAppendTime ) to be ingestion time.
3. *Processing time:* A timestamp set when the data or event record first starts to flow through a processing pipeline.p














